# free_space_segmentation
The project is founded on the precedent set by the following project ["ORFD: Off-Road-Freespace-Detection
"](https://github.com/chaytonmin/Off-Road-Freespace-Detection/tree/main).

## Introduction

The ORFD project is resumed and adapted to facilitate its implementation in the detection of paths in strawberry polytunnels. The repository with modifications to the original code is located at: ["Off-road-detection"](https://github.com/adri-gth/Off-road-detection/tree/main).

### Instruction for running the docker image

 The 'docker' folder contains a shell file. This file must be downloaded to the host and executed using the following commands:
 
 To start the container for the first time, execute the following command:
 
     cd <the directory where the 'obs_detect.sh' file is located>
     ./obs_detect.sh run
     
This command sets up and runs a new container using the 'adriavdocker/obsdetection:v2' image 

To start the container that has already been created: 

    ./obs_detect.sh start
    
To enter the container after it has been started

    ./obs_detect.sh enter
    
To stop the container 

    ./obs_detect.sh stop
    
Additional Notes: Ensure that the 'obs_detect.sh' script has execute permissions. If it does not, you can grant execute permissions using the following command:

    chmod +x obs_detect.sh

### Extracting depth, RGB images and point clouds 

#### Obtaining Data in an Unsynchronized Manner

To extract data in an unsynchronized manner, the launch file get_node_data.launch.py should be executed:

    ros2 launch get_node_data get_node_data.launch.py

For each node, the root path where the information is stored is:

- '/home/data/unsynchronized_dataset/datasets/ORFD/testing/sequence/****'

The folders created to store the information are:

    rgb_image: image_data 
    point Cloud: lidar_data 
    calib: calib 
    depth_image: dense_depth 


#### Obtaining Data in a Synchronized Manner

To obtain information in a synchronized manner (for the ORFD project, this is mandatory), the data_synchronizer node should be executed:
 
    ros2 run my_py_pkg sync_node

The root path where the information is stored is:

- '/home/data/synchronized_dataset/datasets/ORFD/testing/sequence/****'


### Instructions for creating a training dataset

With the folders generated previously, the only thing left to do is to generate gt_image. To accomplish this, the [Supervisely](https://supervisely.com/) platform is used.

To generate the ground truth image data:

1. The 'image_data' folder, generated by the launch file, is uploaded to it.

2. In the platform Supervisely, the 'road' class is created, applicable to 'any shape', which enables the segmentation of paths.

3. After the segmentation is applied to the data, the project is downloaded and exported using the 'Export to Supervisely Format' option, retaining only the .json annotations.
    
4. Subsequently, in the 'python_code' folder, there is an 'extract_mask.py' script. In this script, the path to the file downloaded from Supervisely, the name of that file, and the folder in which the mask will be extracted need to be updated. The script creates the 'gt_image' folder.


The folders 'lidar_data', 'dense_depth', 'image_data', 'calib', and 'gt_image' are already available. They should be organized in the following manner:

```
|-- datasets
 |  |-- ORFD
 |  |  |-- training
 |  |  |  |-- sequence   |-- calib
 |  |  |                 |-- sparse_depth
 |  |  |                 |-- dense_depth
 |  |  |                 |-- lidar_data
 |  |  |                 |-- image_data
 |  |  |                 |-- gt_image
 ......
 |  |  |-- validation
 ......
 |  |  |-- testing
 ......
```
The location of 'datasets', which is the folder containing all the information, must be the same as the path for 'test.sh' and 'train.sh' so that the program can access the information.
