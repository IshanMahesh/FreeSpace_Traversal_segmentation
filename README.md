# free_space_segmentation
The project is founded on the precedent set by the following project ["ORFD: Off-Road-Freespace-Detection
"](https://github.com/chaytonmin/Off-Road-Freespace-Detection/tree/main).

## Introduction

The ORFD project is resumed and adapted to facilitate its implementation in the detection of paths in strawberry polytunnels. The repository with modifications to the original code is located at: ["Off-road-detection"](https://github.com/adri-gth/Off-road-detection/tree/main).

### Instruction for running the docker image
 The 'docker' folder contains a shell file. This file must be downloaded to the host and executed using the following commands:
 
 To start the container for the first time, execute the following command:
 
     cd <the directory where the 'obs_detect.sh' file is located>
     ./obs_detect.sh run
     
This command sets up and runs a new container using the 'adriavdocker/obsdetection:v2

To start the container that has already been created: 

    ./obs_detect.sh start
    
To enter the container after it has been started

    ./obs_detect.sh enter
    
To stop the container 

    ./obs_detect.sh stop
    
Additional Notes: Ensure that the 'obs_detect.sh' script has execute permissions. If it does not, you can grant execute permissions using the following command:

    chmod +x obs_detect.sh

### Extracting depth, RGB images and point clouds 














[Supervisely](https://supervisely.com/).

To generate the ground truth image data, the Supervisely platform is used. The 'image_data' folder, generated by the launch file, is uploaded to it. The 'road' class is created, applicable to 'any shape', which enables the segmentation of paths.

After the segmentation is applied to the data, the project is downloaded and exported using the 'Export to Supervisely Format' option, retaining only the .json annotations. Subsequently, in the 'python_code' folder, there is an 'extract_mask.py' script. In this script, the path to the file downloaded from Supervisely, the name of that file, and the folder in which the mask will be extracted need to be updated. The script creates the 'gt_image' folder.


### Instructions for creating a training dataset

The folders 'lidar_data', 'dense_depth', 'image_data', 'calib', and 'gt_image' are already available. They should be organized in the following manner:


```
|-- datasets
 |  |-- ORFD
 |  |  |-- training
 |  |  |  |-- sequence   |-- calib
 |  |  |                 |-- sparse_depth
 |  |  |                 |-- dense_depth
 |  |  |                 |-- lidar_data
 |  |  |                 |-- image_data
 |  |  |                 |-- gt_image
 ......
 |  |  |-- validation
 ......
 |  |  |-- testing
 ......
```
The location of 'datasets', which is the folder containing all the information, must be the same as the path for 'test.sh' and 'train.sh' so that the program can access the information


































